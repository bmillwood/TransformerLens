{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einsum Divergence\n",
    "\n",
    "For context: https://github.com/TransformerLensOrg/TransformerLens/issues/591"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U transformers\n",
    "! pip install git+https://github.com/TransformerLensOrg/TransformerLens.git@einsum_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! huggingface-cli login # if using models from the Hugging Face Hub that require auth. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up (Based on Chris Mathwin's Gemma Notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformer_lens import HookedTransformer\n",
    "import gc\n",
    "import einops\n",
    "import numpy as np\n",
    "from transformer_lens.utils import get_device\n",
    "\n",
    "device = get_device()\n",
    "torch.set_grad_enabled(False)\n",
    "dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fancy_einsum import einsum  # the suspect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ooms = [10**-i for i in range(1, 10)]\n",
    "\n",
    "\n",
    "def assert_close_for_ooms(a, b, ooms=ooms):\n",
    "    for oom in ooms:\n",
    "        assert torch.allclose(a, b, rtol=oom, atol=oom), f\"Failed for oom={oom}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce without T-Lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate on Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synthetic_data_and_pytorch_default_result(device):\n",
    "    \"\"\"\n",
    "    Returns synthetic data and the result of the operation using PyTorch's einsum.\n",
    "    For this operation:\n",
    "    \"batch pos head_index d_head, head_index d_head d_model -> batch pos d_model\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # For demonstration purposes, I'll define the shapes:\n",
    "    batch_size = 32\n",
    "    pos = 128\n",
    "    num_heads = 8\n",
    "    d_head = 64\n",
    "    d_model = 4096\n",
    "\n",
    "    # Example tensors\n",
    "    z = torch.randn(batch_size, pos, num_heads, d_head)  # [batch, pos, head_index, d_head]\n",
    "    W_O = torch.randn(num_heads, d_head, d_model)        # [head_index, d_head, d_model]\n",
    "    b_O = torch.randn(d_model)                           # [d_model]\n",
    "\n",
    "    device = \"cpu\"\n",
    "    # move all tensors to the device\n",
    "    z = z.to(device)\n",
    "    W_O = W_O.to(device)\n",
    "    b_O = b_O.to(device)\n",
    "    \n",
    "    vanilla_result = (z.flatten(-2,-1) @ W_O.flatten(0,1)) + b_O#.reshape(*z.shape[:-1], -1).shape\n",
    "    \n",
    "    return z, W_O, b_O, vanilla_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128, 4096])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Tensor-likes are not close!\n\nMismatched elements: 14475594 / 16777216 (86.3%)\nGreatest absolute difference: 0.00016021728515625 at index (24, 93, 3020) (up to 1e-07 allowed)\nGreatest relative difference: 5.4666666984558105 at index (21, 19, 1125) (up to 1e-07 allowed)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(out\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# <- Fails on my mac (M3 MAX) on either cpu or mps.\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtesting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_close\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvanilla_result\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-7\u001b[39;49m\u001b[43m)\u001b[49m \n",
      "File \u001b[0;32m~/GithubRepositories/TransformerLenJosephRemote/.venv/lib/python3.9/site-packages/torch/testing/_comparison.py:1520\u001b[0m, in \u001b[0;36massert_close\u001b[0;34m(actual, expected, allow_subclasses, rtol, atol, equal_nan, check_device, check_dtype, check_layout, check_stride, msg)\u001b[0m\n\u001b[1;32m   1498\u001b[0m error_metas \u001b[38;5;241m=\u001b[39m not_close_error_metas(\n\u001b[1;32m   1499\u001b[0m     actual,\n\u001b[1;32m   1500\u001b[0m     expected,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1515\u001b[0m     msg\u001b[38;5;241m=\u001b[39mmsg,\n\u001b[1;32m   1516\u001b[0m )\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_metas:\n\u001b[1;32m   1519\u001b[0m     \u001b[38;5;66;03m# TODO: compose all metas into one AssertionError\u001b[39;00m\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_metas[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto_error(msg)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Tensor-likes are not close!\n\nMismatched elements: 14475594 / 16777216 (86.3%)\nGreatest absolute difference: 0.00016021728515625 at index (24, 93, 3020) (up to 1e-07 allowed)\nGreatest relative difference: 5.4666666984558105 at index (21, 19, 1125) (up to 1e-07 allowed)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from fancy_einsum import einsum\n",
    "# Assume z and self.W_O are given tensors with the correct shapes.\n",
    "\n",
    "z, W_O, b_O, vanilla_result = get_synthetic_data_and_pytorch_default_result(\"cpu\")\n",
    "\n",
    "out = (\n",
    "    (\n",
    "        einsum(\"batch pos head_index d_head, head_index d_head d_model -> batch pos d_model\",\n",
    "            z,\n",
    "            W_O,\n",
    "        )\n",
    "    )\n",
    "    + b_O\n",
    ")  # [batch, pos, d_model]\n",
    "\n",
    "print(out.shape)\n",
    "\n",
    "# <- Fails on my mac (M3 MAX) on either cpu or mps.\n",
    "torch.testing.assert_close(out, vanilla_result, rtol=1e-7, atol=1e-7) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that if we don't use einsum, just einops, this works. Why??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But if we use fancy_opt_einsum (combination of opt_einsum and einops, it passes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128, 4096])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "z, W_O, b_O, vanilla = get_synthetic_data_and_pytorch_default_result(\"cpu\")\n",
    "\n",
    "out = (\n",
    "    (\n",
    "        torch.einsum(\"bpij,ijk->bpk\",\n",
    "            z,\n",
    "            W_O,\n",
    "        )\n",
    "    )\n",
    "    + b_O\n",
    ")  # [batch, pos, d_model]\n",
    "\n",
    "print(out.shape)\n",
    "\n",
    "\n",
    "vanilla = (z.flatten(-2,-1) @ W_O.flatten(0,1)) + b_O#.reshape(*z.shape[:-1], -1).shape\n",
    "torch.testing.assert_close(out, vanilla, rtol=1e-9, atol=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First show just opt_einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128, 4096])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from opt_einsum import contract\n",
    "\n",
    "z, W_O, b_O, vanilla = get_synthetic_data_and_pytorch_default_result(\"cpu\")\n",
    "\n",
    "out = (\n",
    "    (\n",
    "        contract(\"bpij,ijk->bpk\",\n",
    "            z,\n",
    "            W_O,\n",
    "        )\n",
    "    )\n",
    "    + b_O\n",
    ")  # [batch, pos, d_model]\n",
    "\n",
    "print(out.shape)\n",
    "\n",
    "\n",
    "vanilla = (z.flatten(-2,-1) @ W_O.flatten(0,1)) + b_O#.reshape(*z.shape[:-1], -1).shape\n",
    "torch.testing.assert_close(out, vanilla, rtol=1e-9, atol=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### then show the hybrid with original syntax from fancy einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128, 4096])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from fancy_einsum import convert_equation\n",
    "from opt_einsum import contract\n",
    "\n",
    "def fancy_opt_einsum(equation: str, *operands):\n",
    "    \"\"\"\n",
    "    Variation on fancy opt einsum that uses opt_einsum for the contraction.\n",
    "    \n",
    "    Evaluates the Einstein summation convention on the operands.\n",
    "    \n",
    "    See: \n",
    "      https://pytorch.org/docs/stable/generated/torch.einsum.html\n",
    "      https://numpy.org/doc/stable/reference/generated/numpy.einsum.html\n",
    "    \"\"\"\n",
    "    new_equation = convert_equation(equation)\n",
    "    return contract(new_equation, *operands)\n",
    "\n",
    "z, W_O, b_O, vanilla = get_synthetic_data_and_pytorch_default_result(\"cpu\")\n",
    "\n",
    "out = (\n",
    "    (\n",
    "        fancy_opt_einsum(\"batch pos head_index d_head, head_index d_head d_model -> batch pos d_model\",\n",
    "            z,\n",
    "            W_O,\n",
    "        )\n",
    "    )\n",
    "    + b_O\n",
    ")  # [batch, pos, d_model]\n",
    "\n",
    "\n",
    "print(out.shape)\n",
    "vanilla = (z.flatten(-2,-1) @ W_O.flatten(0,1)) + b_O#.reshape(*z.shape[:-1], -1).shape\n",
    "torch.testing.assert_close(out, vanilla, rtol=1e-9, atol=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Impact on Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingface Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josephbloom/GithubRepositories/TransformerLenJosephRemote/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183706bd256742aa9d62be2589ce66b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "model_name = \"google/gemma-2b\"\n",
    "\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=torch.float32\n",
    ")  # trust_remote_code=True, attn_implementation=\"eager\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name\n",
    ")  # add_bos_token = True, use_fast=False, trust_remote_code=True)\n",
    "hf_model.eval().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-Lens Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "982ab603ecdd4b08b0619a0333685863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "hooked_model = HookedTransformer.from_pretrained(\n",
    "    model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    fold_ln=False,\n",
    "    fold_value_biases=False,\n",
    "    center_writing_weights=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Forward Passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "TransformerLens lets you load in 50+ different open source language models,\n",
    "and exposes the internal activations of the model to you. You can cache\n",
    "any internal activation in the model, and add in functions to edit, remove\n",
    "or replace these activations as the model runs.\n",
    "\"\"\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"].to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run each model with a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GemmaModel is using GemmaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    hf_outputs = hf_model(input_ids, output_hidden_states=True, output_attentions=True)\n",
    "    hf_logits_cpu = hf_outputs[\"logits\"].cpu()\n",
    "    hf_resid_pre_cache = hf_outputs[\"hidden_states\"]\n",
    "    hf_attentions = hf_outputs[\"attentions\"]\n",
    "    hf_resid_pre_cache_cpu = [cache.cpu() for cache in hf_resid_pre_cache]\n",
    "    hf_attentions_cpu = [att.cpu() for att in hf_attentions]\n",
    "    hf_outputs = hf_model(input_ids, labels=input_ids)\n",
    "    hf_loss_cpu = hf_outputs.loss.cpu()\n",
    "\n",
    "# TODO: add a some notebook config for low memory mode.\n",
    "# del hf_model\n",
    "# del hf_outputs\n",
    "# del hf_resid_pre_cache\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "with torch.no_grad():\n",
    "    hooked_model_logits, hooked_model_cache = hooked_model.run_with_cache(input_ids)\n",
    "    hooked_model_loss = hooked_model(input_ids, return_type=\"loss\")\n",
    "    hooked_model_loss_cpu = hooked_model_loss.cpu()\n",
    "    hooked_model_logits_cpu = hooked_model_logits.detach().cpu()\n",
    "    hooked_model_cache_cpu = {k: v.cpu() for k, v in hooked_model_cache.items()}\n",
    "    n_layers = hooked_model.cfg.n_layers\n",
    "\n",
    "# TODO: add a some notebook config for low memory mode.\n",
    "# # del hooked_model\n",
    "# del hooked_model_logits\n",
    "# del hooked_model_cache\n",
    "# del hooked_model_loss\n",
    "\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "assert_close_for_ooms(hf_logits_cpu, hooked_model_logits_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Matching hf and T-Lens residual stream in between transformer blocks *****\n",
      "***** \ttesting with atol=0.0001 and rtol=0.0001\t *****\n",
      "layer 1 \t not close, max difference: 22.35356903076172\n",
      "layer 2 \t not close, max difference: 44.470909118652344\n",
      "layer 3 \t not close, max difference: 56.772979736328125\n",
      "layer 4 \t not close, max difference: 68.3115463256836\n",
      "layer 5 \t not close, max difference: 82.96070861816406\n",
      "layer 6 \t not close, max difference: 103.59776306152344\n",
      "layer 7 \t not close, max difference: 119.89836883544922\n",
      "layer 8 \t not close, max difference: 243.830322265625\n",
      "layer 9 \t not close, max difference: 494.560546875\n",
      "layer 10 \t not close, max difference: 495.130859375\n",
      "layer 11 \t not close, max difference: 494.717041015625\n",
      "layer 12 \t not close, max difference: 493.398681640625\n",
      "layer 13 \t not close, max difference: 491.9833984375\n",
      "layer 14 \t not close, max difference: 490.833984375\n",
      "layer 15 \t not close, max difference: 488.646484375\n",
      "layer 16 \t not close, max difference: 481.8250732421875\n",
      "layer 17 \t not close, max difference: 416.03564453125\n",
      "All layers match\n"
     ]
    }
   ],
   "source": [
    "pass_loose_bound = True\n",
    "print(\"*\"*5, \"Matching hf and T-Lens residual stream in between transformer blocks\", \"*\"*5)\n",
    "atol = rtol = 1e-4\n",
    "print(\"*\"*5, f\"\\ttesting with {atol=} and {rtol=}\\t\",\"*\"*5)\n",
    "for l in range(n_layers):\n",
    "    try:\n",
    "        torch.testing.assert_close(hooked_model_cache_cpu[f'blocks.{l}.hook_resid_pre'], hf_resid_pre_cache_cpu[l], atol=atol, rtol=rtol)\n",
    "    except:\n",
    "        max_diff = (hooked_model_cache_cpu[f'blocks.{l}.hook_resid_pre'] - hf_resid_pre_cache_cpu[l]).abs().max()\n",
    "        print(f\"layer {l} \\t not close, max difference: {max_diff}\")\n",
    "        pass_loose_bound = False\n",
    "\n",
    "if pass_loose_bound:\n",
    "    print(f\"All layers match with {atol=} {rtol=}\")\n",
    "else:\n",
    "    print(\"All layers match\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
